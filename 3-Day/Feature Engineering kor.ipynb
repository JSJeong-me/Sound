{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 특성 엔지니어링 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oakyo\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "# from sklearn.externals import joblib\n",
    "import joblib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = './archive/'\n",
    "files = glob.glob(ROOT_DIR+'/**/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0].split('\\\\')[-1].split('-')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_windows : 오디오 하위 샘플의 시작 및 종료 색을을 가져오는 함수_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sound_data(path, sr=22050):\n",
    "    data, fsr = sf.read(path)\n",
    "    data_resample = librosa.resample(data.T, fsr, sr)\n",
    "    if len(data_resample.shape) > 1:\n",
    "        data_resample = np.average(data_resample, axis=0)\n",
    "    return data_resample, sr\n",
    "\n",
    "def windows(data, window_size):\n",
    "    start = 0\n",
    "    while start < len(data):\n",
    "        yield int(start), int(start + window_size)\n",
    "        start += (window_size / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, sr = get_sound_data(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88200, 22050)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d), sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32256 / 22050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract_features\n",
    "- _하기 이미지와 같이 특성 추출을 합니다._\n",
    "![feature_extraction](assets/feature_extraction.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_names, bands=64, frames=64):\n",
    "    \n",
    "    window_size = 512 * (frames - 1)  \n",
    "    log_specgrams_full = []\n",
    "    log_specgrams_hp = []\n",
    "    class_labels = []\n",
    "    # for each audio sample\n",
    "    for fn in file_names:\n",
    "        file_name = fn.split('\\\\')[-1]\n",
    "        class_label = file_name.split('-')[1]\n",
    "        sound_data, sr = get_sound_data(fn, sr=22050)\n",
    "        # for each audio signal sub-sample window of data\n",
    "        for (start,end) in windows(sound_data, window_size):\n",
    "            if(len(sound_data[start:end]) == window_size):\n",
    "                signal = sound_data[start:end]\n",
    "                # get the log-scaled mel-spectrogram\n",
    "                melspec_full = librosa.feature.melspectrogram(signal, n_mels = bands)\n",
    "                logspec_full = librosa.amplitude_to_db(melspec_full)\n",
    "                logspec_full = logspec_full.T.flatten()[:, np.newaxis].T\n",
    "                # get the log-scaled, averaged values for the harmonic & percussive components\n",
    "                y_harmonic, y_percussive = librosa.effects.hpss(signal)\n",
    "                melspec_harmonic = librosa.feature.melspectrogram(y_harmonic, n_mels = bands)\n",
    "                melspec_percussive = librosa.feature.melspectrogram(y_percussive, n_mels = bands)\n",
    "                logspec_harmonic = librosa.amplitude_to_db(melspec_harmonic)\n",
    "                logspec_percussive = librosa.amplitude_to_db(melspec_percussive)\n",
    "                logspec_harmonic = logspec_harmonic.T.flatten()[:, np.newaxis].T\n",
    "                logspec_percussive = logspec_percussive.T.flatten()[:, np.newaxis].T\n",
    "                logspec_hp = np.average([logspec_harmonic, logspec_percussive], axis=0)\n",
    "                \n",
    "                log_specgrams_full.append(logspec_full)\n",
    "                log_specgrams_hp.append(logspec_hp)\n",
    "                class_labels.append(class_label)\n",
    "    # create the first two feature maps            \n",
    "    log_specgrams_full = np.asarray(log_specgrams_full).reshape(len(log_specgrams_full), bands ,frames, 1)\n",
    "    log_specgrams_hp = np.asarray(log_specgrams_hp).reshape(len(log_specgrams_hp), bands ,frames, 1)\n",
    "    features = np.concatenate((log_specgrams_full, \n",
    "                               log_specgrams_hp, \n",
    "                               np.zeros(np.shape(log_specgrams_full))), \n",
    "                              axis=3)\n",
    "    # create the third feature map which is the delta (derivative) of the log-scaled mel-spectrogram\n",
    "    for i in range(len(features)):\n",
    "        features[i, :, :, 2] = librosa.feature.delta(features[i, :, :, 0])\n",
    "    \n",
    "    return np.array(features), np.array(class_labels, dtype = np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = extract_features(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_64 x 64 x 3으로 특성 추출된 것을 확인 할 수 있습니다._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {'0' : 'air_conditioner', '1' : 'car_horn', '2' : 'children_playing', '3' : 'dog_bark', '4' : 'drilling', \n",
    "                 '5' : 'engine_idling', '6' : 'gun_shot', '7' : 'jackhammer', '8' : 'siren', '9' : 'street_music'}\n",
    "\n",
    "categories = list(set(labels))\n",
    "sample_idxs = [np.where(labels == label_id)[0][0] for label_id in categories]\n",
    "feature_samples = features[sample_idxs]\n",
    "feature_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_3가지 특성 데이터를 imshow를 사용하여 이미지로 보이도록 합니다._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "for index, (feature_map, category) in enumerate(zip(feature_samples, categories)):\n",
    "    plt.subplot(2, 5, index+1)\n",
    "    plt.imshow(np.concatenate((feature_map[:,:,0], feature_map[:,:,1], feature_map[:,:,2]), axis=1), cmap='viridis')\n",
    "    plt.title(class_map[str(category)])\n",
    "plt.tight_layout()\n",
    "t = plt.suptitle('Visualizing Feature Maps for Audio Clips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset_labels.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(features, 'base_features.pkl')\n",
    "joblib.dump(labels, 'dataset_labels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30500, 64, 64, 3), (30500,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = joblib.load('base_features.pkl')\n",
    "labels = joblib.load('dataset_labels.pkl')\n",
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30500, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(list(zip(features, labels)))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_훈련, 검증, 테스트 데이터 구성_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = np.split(data, [int(.6*len(data)), int(.8*len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18300, 2), (6100, 2), (6100, 2))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Counter({0: 2427, 9: 2414, 2: 2379, 5: 2323, 8: 2118, 7: 2098, 4: 2062, 3: 1735, 1: 539, 6: 205}) \n",
      "Validate: Counter({5: 854, 9: 798, 0: 783, 8: 736, 2: 723, 7: 712, 4: 660, 3: 579, 1: 184, 6: 71}) \n",
      "Test: Counter({2: 845, 9: 788, 0: 783, 8: 757, 5: 733, 4: 683, 7: 663, 3: 598, 1: 190, 6: 60})\n"
     ]
    }
   ],
   "source": [
    "print('Train:', Counter(item[1] for item in train), \n",
    "      '\\nValidate:', Counter(item[1] for item in validate), \n",
    "      '\\nTest:',Counter(item[1] for item in test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[vgg ref](https://arxiv.org/abs/1409.1556)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sound_data(data):\n",
    "    data = np.expand_dims(data, axis=0)\n",
    "    data = preprocess_input(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _include_top을 false로 하여 fully connected를 포함하지 않습니다._\n",
    "- _모델 output에 max pooling을 flatten하였습니다. _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import vgg16\n",
    "from keras.models import Model\n",
    "import keras\n",
    "\n",
    "vgg = vgg16.VGG16(include_top=False, weights='imagenet', \n",
    "                                     input_shape=(64, 64, 3))\n",
    "\n",
    "\n",
    "output = vgg.layers[-1].output\n",
    "output = keras.layers.Flatten()(output)\n",
    "\n",
    "model = Model(vgg.input, output)\n",
    "model.trainable = False\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_특성 추출을 수행합니다._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tl_features(model, base_feature_data):\n",
    "    dataset_tl_features = []\n",
    "    for index, feature_data in enumerate(base_feature_data):\n",
    "        if (index+1) % 1000 == 0:\n",
    "            print('Finished processing', index+1, 'sound feature maps')\n",
    "        pr_data = process_sound_data(feature_data)\n",
    "        tl_features = model.predict(pr_data)\n",
    "        tl_features = np.reshape(tl_features, tl_features.shape[1])\n",
    "        dataset_tl_features.append(tl_features)\n",
    "    return np.array(dataset_tl_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 1000 sound feature maps\n",
      "Finished processing 2000 sound feature maps\n",
      "Finished processing 3000 sound feature maps\n",
      "Finished processing 4000 sound feature maps\n",
      "Finished processing 5000 sound feature maps\n",
      "Finished processing 6000 sound feature maps\n",
      "Finished processing 7000 sound feature maps\n",
      "Finished processing 8000 sound feature maps\n",
      "Finished processing 9000 sound feature maps\n",
      "Finished processing 10000 sound feature maps\n",
      "Finished processing 11000 sound feature maps\n",
      "Finished processing 12000 sound feature maps\n",
      "Finished processing 13000 sound feature maps\n",
      "Finished processing 14000 sound feature maps\n",
      "Finished processing 15000 sound feature maps\n",
      "Finished processing 16000 sound feature maps\n",
      "Finished processing 17000 sound feature maps\n",
      "Finished processing 18000 sound feature maps\n"
     ]
    }
   ],
   "source": [
    "train_base_features = [item[0] for item in train]\n",
    "train_labels = np.array([item[1] for item in train])\n",
    "train_tl_features = extract_tl_features(model=model, base_feature_data=train_base_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_labels.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(train_tl_features, 'train_tl_features.pkl')\n",
    "joblib.dump(train_labels, 'train_labels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18300, 2048), (18300,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tl_features.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 1000 sound feature maps\n",
      "Finished processing 2000 sound feature maps\n",
      "Finished processing 3000 sound feature maps\n",
      "Finished processing 4000 sound feature maps\n",
      "Finished processing 5000 sound feature maps\n",
      "Finished processing 6000 sound feature maps\n"
     ]
    }
   ],
   "source": [
    "validate_base_features = [item[0] for item in validate]\n",
    "validate_labels = np.array([item[1] for item in validate])\n",
    "validate_tl_features = extract_tl_features(model=model, base_feature_data=validate_base_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['validate_labels.pkl']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(validate_tl_features, 'validate_tl_features.pkl')\n",
    "joblib.dump(validate_labels, 'validate_labels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 1000 sound feature maps\n",
      "Finished processing 2000 sound feature maps\n",
      "Finished processing 3000 sound feature maps\n",
      "Finished processing 4000 sound feature maps\n",
      "Finished processing 5000 sound feature maps\n",
      "Finished processing 6000 sound feature maps\n"
     ]
    }
   ],
   "source": [
    "test_base_features = [item[0] for item in test]\n",
    "test_labels = np.array([item[1] for item in test])\n",
    "test_tl_features = extract_tl_features(model=model, base_feature_data=test_base_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_labels.pkl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(test_tl_features, 'test_tl_features.pkl')\n",
    "joblib.dump(test_labels, 'test_labels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18300, 2048), (6100, 2048), (6100, 2048))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tl_features.shape, validate_tl_features.shape, test_tl_features.shape"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
